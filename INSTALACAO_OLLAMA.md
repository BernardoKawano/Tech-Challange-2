# ü§ñ GUIA DE INSTALA√á√ÉO - OLLAMA (LLM LOCAL)

**Data:** 15 de Outubro de 2025  
**Op√ß√£o Escolhida:** Ollama Local (100% GR√ÅTIS)

---

## üìã PR√â-REQUISITOS

- ‚úÖ Windows 10/11
- ‚úÖ 8GB+ RAM (ideal: 16GB)
- ‚úÖ ~4GB espa√ßo em disco (para o modelo)
- ‚úÖ Internet (s√≥ para download inicial)

---

## üì• PASSO 1: BAIXAR OLLAMA

### **Op√ß√£o A: Instalador Windows (RECOMENDADO)**

1. Acesse: https://ollama.ai/download/windows
2. Baixe o instalador: `OllamaSetup.exe`
3. Execute como administrador
4. Siga o assistente de instala√ß√£o
5. Pronto!

### **Op√ß√£o B: Via PowerShell**

```powershell
# Baixar e instalar
winget install Ollama.Ollama
```

---

## ‚úÖ PASSO 2: VERIFICAR INSTALA√á√ÉO

Abra o PowerShell e digite:

```powershell
ollama --version
```

**Sa√≠da esperada:**
```
ollama version is 0.x.x
```

---

## ü§ñ PASSO 3: BAIXAR MODELO

Escolha um modelo (recomendo come√ßar com o menor):

### **Op√ß√£o A: Llama 2 7B (RECOMENDADO)** 
- Tamanho: ~3.8GB
- RAM: 8GB
- Qualidade: Boa
- Velocidade: R√°pida

```powershell
ollama pull llama2
```

### **Op√ß√£o B: Mistral 7B** 
- Tamanho: ~4.1GB
- RAM: 8GB
- Qualidade: Muito boa
- Velocidade: R√°pida

```powershell
ollama pull mistral
```

### **Op√ß√£o C: Llama 2 13B** 
- Tamanho: ~7.3GB
- RAM: 16GB
- Qualidade: Excelente
- Velocidade: Mais lenta

```powershell
ollama pull llama2:13b
```

**ATEN√á√ÉO:** O download pode demorar 10-30 minutos dependendo da sua internet!

---

## üß™ PASSO 4: TESTAR

Teste se o modelo est√° funcionando:

```powershell
ollama run llama2 "Ol√°, como voc√™ est√°?"
```

**Sa√≠da esperada:**
```
Ol√°! Estou funcionando bem, obrigado por perguntar. Como posso ajud√°-lo hoje?
```

Para sair do chat, digite: `/bye`

---

## üîß PASSO 5: INICIAR SERVIDOR

O Ollama precisa estar rodando em background. Execute:

```powershell
ollama serve
```

**DEIXE ESTA JANELA ABERTA!** O servidor precisa estar rodando para o Python se conectar.

**Ou** configure para iniciar automaticamente:
1. Pressione `Win + R`
2. Digite: `shell:startup`
3. Crie um atalho para: `C:\Program Files\Ollama\ollama.exe serve`

---

## üì¶ PASSO 6: INSTALAR BIBLIOTECA PYTHON

No seu projeto:

```powershell
cd "Tech Challange #2\Tech-Challange-2"
pip install ollama
```

---

## ‚úÖ PASSO 7: TESTAR INTEGRA√á√ÉO PYTHON

Crie um arquivo de teste:

```python
# test_ollama.py
import ollama

response = ollama.chat(model='llama2', messages=[
  {
    'role': 'user',
    'content': 'Por favor, diga ol√° em uma frase curta.',
  },
])
print(response['message']['content'])
```

Execute:
```powershell
python test_ollama.py
```

**Sa√≠da esperada:**
```
Ol√°! Como posso ajud√°-lo hoje?
```

---

## üéõÔ∏è COMANDOS √öTEIS

### **Ver modelos instalados:**
```powershell
ollama list
```

### **Remover modelo (liberar espa√ßo):**
```powershell
ollama rm llama2
```

### **Ver status do servidor:**
```powershell
ollama ps
```

### **Parar servidor:**
```powershell
# Ctrl+C na janela do servidor
```

---

## üêõ TROUBLESHOOTING

### **Erro: "ollama: command not found"**
- Reinicie o PowerShell/Terminal
- Verifique se instalou corretamente
- Tente reiniciar o PC

### **Erro: "Failed to load model"**
- Certifique-se que o servidor est√° rodando: `ollama serve`
- Verifique se o modelo foi baixado: `ollama list`
- Tente baixar novamente: `ollama pull llama2`

### **Muito lento ou trava**
- Seu PC pode n√£o ter RAM suficiente
- Tente um modelo menor (llama2:7b em vez de 13b)
- Feche outros programas

### **Erro: "Connection refused"**
- O servidor n√£o est√° rodando
- Execute: `ollama serve` em outra janela

---

## üìä COMPARA√á√ÉO DE MODELOS

| Modelo | Tamanho | RAM M√≠n. | Qualidade | Velocidade | Uso |
|--------|---------|----------|-----------|------------|-----|
| llama2 | 3.8GB | 8GB | Boa | R√°pida | Geral |
| mistral | 4.1GB | 8GB | Muito Boa | R√°pida | Recomendado |
| llama2:13b | 7.3GB | 16GB | Excelente | Lenta | Qualidade m√°xima |
| codellama | 3.8GB | 8GB | Boa | R√°pida | C√≥digo |
| phi | 1.6GB | 4GB | OK | Muito R√°pida | PC fraco |

---

## üéØ CHECKLIST DE INSTALA√á√ÉO

- [ ] Baixar e instalar Ollama
- [ ] Verificar instala√ß√£o (`ollama --version`)
- [ ] Baixar modelo (`ollama pull llama2`)
- [ ] Testar modelo (`ollama run llama2 "teste"`)
- [ ] Iniciar servidor (`ollama serve`)
- [ ] Instalar biblioteca Python (`pip install ollama`)
- [ ] Testar integra√ß√£o Python
- [ ] ‚úÖ PRONTO PARA USAR!

---

## üí° DICAS

1. **Primeira gera√ß√£o √© sempre mais lenta** (modelo carrega na mem√≥ria)
2. **Deixe o servidor rodando** durante todo o uso
3. **Modelos menores s√£o mais r√°pidos** mas menos precisos
4. **Se travar, reinicie** o servidor Ollama
5. **Monitor de desempenho** pode mostrar uso de RAM/CPU

---

## üîó LINKS √öTEIS

- Site oficial: https://ollama.ai/
- Documenta√ß√£o: https://github.com/ollama/ollama
- Modelos dispon√≠veis: https://ollama.ai/library
- Python client: https://github.com/ollama/ollama-python

---

## ‚è±Ô∏è TEMPO ESTIMADO

- Download Ollama: 2-5 minutos
- Instala√ß√£o: 1-2 minutos
- Download modelo: 10-30 minutos (depende da internet)
- Testes: 2-5 minutos

**TOTAL: ~20-40 minutos**

---

## üéâ PRONTO!

Ap√≥s completar todos os passos, voc√™ ter√°:
- ‚úÖ Ollama instalado
- ‚úÖ Modelo baixado
- ‚úÖ Servidor rodando
- ‚úÖ Python conectado
- ‚úÖ Pronto para gerar instru√ß√µes e relat√≥rios!

**Pr√≥ximo passo:** Executar o sistema completo com LLM! üöÄ

---

**Criado em:** 15/10/2025  
**Atualizado em:** 15/10/2025  
**Vers√£o:** 1.0


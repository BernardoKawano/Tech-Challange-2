"""
Sistema de Perguntas e Respostas sobre Rotas usando LLM.

Permite que usu√°rios fa√ßam perguntas em linguagem natural sobre:
- Rotas otimizadas
- Entregas
- Ve√≠culos
- M√©tricas
- Sugest√µes de melhorias
"""
import os
from typing import List, Dict, Optional
from datetime import datetime


class QASystem:
    """Sistema de Q&A para consultas sobre rotas em linguagem natural."""
    
    def __init__(self, provider: str = "ollama", model: str = None, api_key: str = None):
        """
        Inicializa o sistema de Q&A.
        
        Args:
            provider: "ollama" (local, gr√°tis) ou "openai" (nuvem, pago)
            model: Nome do modelo
            api_key: API key (apenas para OpenAI)
        """
        self.provider = provider.lower()
        self.context = {}  # Contexto das rotas para consultas
        
        if self.provider == "ollama":
            try:
                import ollama
                self.client = ollama
                self.model = model or "llama2"
                print(f"‚úÖ Sistema Q&A configurado com Ollama: {self.model}")
            except ImportError:
                raise ImportError(
                    "Ollama n√£o instalado! Execute: pip install ollama\n"
                    "E instale o Ollama: https://ollama.ai/download/windows"
                )
        
        elif self.provider == "openai":
            try:
                from openai import OpenAI
                self.client = OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))
                self.model = model or "gpt-3.5-turbo"
                print(f"‚úÖ Sistema Q&A configurado com OpenAI: {self.model}")
            except ImportError:
                raise ImportError("OpenAI n√£o instalado! Execute: pip install openai")
        
        else:
            raise ValueError(f"Provider '{provider}' n√£o suportado. Use 'ollama' ou 'openai'")
        
        # Hist√≥rico de conversa√ß√£o
        self.conversation_history = []
    
    def load_context(
        self,
        routes: List[Dict],
        vehicles: List[Dict],
        delivery_points: List[Dict],
        metrics: Dict,
        ga_stats: Optional[Dict] = None
    ):
        """
        Carrega o contexto das rotas para consultas.
        
        Args:
            routes: Lista de rotas otimizadas
            vehicles: Lista de ve√≠culos
            delivery_points: Lista de pontos de entrega
            metrics: M√©tricas gerais
            ga_stats: Estat√≠sticas do AG (opcional)
        """
        self.context = {
            'routes': routes,
            'vehicles': vehicles,
            'delivery_points': delivery_points,
            'metrics': metrics,
            'ga_stats': ga_stats or {},
            'loaded_at': datetime.now().isoformat()
        }
        
        print(f"‚úÖ Contexto carregado:")
        print(f"   ‚Ä¢ {len(routes)} rotas")
        print(f"   ‚Ä¢ {len(vehicles)} ve√≠culos")
        print(f"   ‚Ä¢ {len(delivery_points)} pontos de entrega")
    
    def ask(self, question: str) -> str:
        """
        Faz uma pergunta em linguagem natural sobre as rotas.
        
        Args:
            question: Pergunta do usu√°rio
            
        Returns:
            Resposta da LLM
        """
        if not self.context:
            return "‚ùå Erro: Nenhum contexto carregado. Execute load_context() primeiro."
        
        print(f"\nüí¨ Pergunta: {question}")
        
        # Construir prompt com contexto
        prompt = self._build_qa_prompt(question)
        
        # Chamar LLM
        try:
            if self.provider == "ollama":
                response = self._call_ollama(prompt, question)
            else:  # openai
                response = self._call_openai(prompt, question)
            
            # Adicionar ao hist√≥rico
            self.conversation_history.append({
                'question': question,
                'answer': response,
                'timestamp': datetime.now().isoformat()
            })
            
            return response
        
        except Exception as e:
            error_msg = f"‚ùå Erro ao processar pergunta: {e}"
            print(error_msg)
            return error_msg
    
    def _call_ollama(self, prompt: str, question: str) -> str:
        """Chama Ollama local."""
        messages = [
            {
                'role': 'system',
                'content': (
                    'Voc√™ √© um assistente especializado em log√≠stica e otimiza√ß√£o de rotas. '
                    'Responda perguntas sobre rotas, entregas, ve√≠culos e m√©tricas de forma '
                    'CLARA, CONCISA e PRECISA. Use dados do contexto fornecido. '
                    'Se a pergunta n√£o puder ser respondida com os dados dispon√≠veis, '
                    'seja honesto e sugira alternativas. Seja profissional mas amig√°vel.'
                )
            }
        ]
        
        # Adicionar hist√≥rico recente (√∫ltimas 3 perguntas)
        for conv in self.conversation_history[-3:]:
            messages.append({'role': 'user', 'content': conv['question']})
            messages.append({'role': 'assistant', 'content': conv['answer']})
        
        # Adicionar pergunta atual
        messages.append({'role': 'user', 'content': prompt})
        
        response = self.client.chat(model=self.model, messages=messages)
        return response['message']['content']
    
    def _call_openai(self, prompt: str, question: str) -> str:
        """Chama OpenAI API."""
        messages = [
            {
                "role": "system",
                "content": (
                    "Voc√™ √© um assistente especializado em log√≠stica e otimiza√ß√£o de rotas. "
                    "Responda perguntas sobre rotas, entregas, ve√≠culos e m√©tricas de forma "
                    "CLARA, CONCISA e PRECISA. Use dados do contexto fornecido. "
                    "Se a pergunta n√£o puder ser respondida com os dados dispon√≠veis, "
                    "seja honesto e sugira alternativas. Seja profissional mas amig√°vel."
                )
            }
        ]
        
        # Adicionar hist√≥rico recente
        for conv in self.conversation_history[-3:]:
            messages.append({"role": "user", "content": conv['question']})
            messages.append({"role": "assistant", "content": conv['answer']})
        
        # Adicionar pergunta atual
        messages.append({"role": "user", "content": prompt})
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7,
            max_tokens=1000
        )
        
        return response.choices[0].message.content
    
    def _build_qa_prompt(self, question: str) -> str:
        """Constr√≥i prompt com contexto para Q&A."""
        
        # Resumo das rotas
        routes_summary = []
        for i, route in enumerate(self.context['routes'], 1):
            vehicle_name = route.get('vehicle_name', f'Ve√≠culo {i}')
            num_points = len(route.get('points', []))
            distance = route.get('distance_km', 0)
            capacity_usage = route.get('capacity_usage_percent', 0)
            
            routes_summary.append(
                f"  ‚Ä¢ {vehicle_name}: {num_points} entregas, "
                f"{distance:.2f} km, {capacity_usage:.1f}% capacidade"
            )
        
        # Pontos de entrega cr√≠ticos
        critical_points = [
            p for p in self.context['delivery_points']
            if p.get('priority', '').lower() == 'critico'
        ]
        
        # Construir prompt
        prompt = f"""
CONTEXTO SOBRE AS ROTAS OTIMIZADAS:

RESUMO GERAL:
- Total de ve√≠culos: {len(self.context['vehicles'])}
- Total de rotas: {len(self.context['routes'])}
- Total de pontos de entrega: {len(self.context['delivery_points'])}
- Dist√¢ncia total: {self.context['metrics'].get('total_distance', 'N/A')} km
- Entregas cr√≠ticas: {len(critical_points)}

DETALHES DAS ROTAS:
{chr(10).join(routes_summary)}

M√âTRICAS DE DESEMPENHO:
- Fitness final: {self.context['metrics'].get('fitness', 'N/A')}
- Viola√ß√µes de capacidade: {self.context['metrics'].get('capacity_violations', 0)}
- Viola√ß√µes de autonomia: {self.context['metrics'].get('autonomy_violations', 0)}

ALGORITMO GEN√âTICO:
- Gera√ß√µes: {self.context['ga_stats'].get('total_generations', 'N/A')}
- Crossovers: {self.context['ga_stats'].get('total_crossovers', 'N/A')}
- Muta√ß√µes: {self.context['ga_stats'].get('total_mutations', 'N/A')}

PERGUNTA DO USU√ÅRIO:
{question}

INSTRU√á√ïES:
- Responda de forma direta e baseada nos dados acima
- Use n√∫meros e porcentagens quando relevante
- Se precisar de mais informa√ß√µes, seja espec√≠fico sobre o que falta
- Sugira melhorias quando apropriado
- Mantenha a resposta concisa (m√°ximo 200 palavras)
"""
        
        return prompt
    
    def suggest_improvements(self) -> str:
        """
        Gera sugest√µes de melhorias baseadas nos padr√µes identificados.
        
        Returns:
            String com sugest√µes de melhorias
        """
        if not self.context:
            return "‚ùå Erro: Nenhum contexto carregado."
        
        question = """
        Com base nos dados de otimiza√ß√£o fornecidos, analise os padr√µes e identifique:
        
        1. PROBLEMAS DETECTADOS:
           - Rotas desbalanceadas?
           - Ve√≠culos subutilizados ou sobrecarregados?
           - Entregas cr√≠ticas em risco?
           - Dist√¢ncias muito longas?
        
        2. SUGEST√ïES DE MELHORIA:
           - Ajustes na frota (mais/menos ve√≠culos)?
           - Mudan√ßas de capacidade?
           - Altera√ß√£o de prioridades?
           - Redistribui√ß√£o de entregas?
        
        3. OPORTUNIDADES:
           - Redu√ß√£o de custos?
           - Otimiza√ß√£o de tempo?
           - Melhoria no atendimento?
        
        Seja espec√≠fico e pr√°tico. Priorize as 3 melhorias mais impactantes.
        """
        
        print("\nüí° Gerando sugest√µes de melhoria...")
        response = self.ask(question)
        
        return response
    
    def get_route_summary(self, vehicle_name: Optional[str] = None) -> str:
        """
        Retorna resumo de uma rota espec√≠fica ou todas.
        
        Args:
            vehicle_name: Nome do ve√≠culo (None = todas)
        """
        if vehicle_name:
            question = f"Me d√™ um resumo detalhado da rota do ve√≠culo '{vehicle_name}'."
        else:
            question = "Me d√™ um resumo geral de todas as rotas otimizadas."
        
        return self.ask(question)
    
    def compare_routes(self, vehicle1: str, vehicle2: str) -> str:
        """
        Compara duas rotas.
        
        Args:
            vehicle1: Nome do primeiro ve√≠culo
            vehicle2: Nome do segundo ve√≠culo
        """
        question = f"Compare as rotas dos ve√≠culos '{vehicle1}' e '{vehicle2}'. Quais s√£o as principais diferen√ßas?"
        return self.ask(question)
    
    def find_bottlenecks(self) -> str:
        """Identifica gargalos e pontos de aten√ß√£o."""
        question = """
        Analise as rotas e identifique poss√≠veis gargalos:
        - Quais ve√≠culos est√£o pr√≥ximos do limite de capacidade ou autonomia?
        - Quais entregas cr√≠ticas est√£o em rotas longas?
        - H√° algum desbalanceamento significativo entre ve√≠culos?
        """
        return self.ask(question)
    
    def get_conversation_history(self) -> List[Dict]:
        """Retorna hist√≥rico de perguntas e respostas."""
        return self.conversation_history
    
    def clear_history(self):
        """Limpa o hist√≥rico de conversa√ß√£o."""
        self.conversation_history = []
        print("‚úÖ Hist√≥rico de conversa√ß√£o limpo.")
    
    def export_qa_log(self, filepath: str):
        """
        Exporta o hist√≥rico de Q&A para arquivo.
        
        Args:
            filepath: Caminho do arquivo de sa√≠da
        """
        if not self.conversation_history:
            print("‚ö†Ô∏è Nenhuma conversa√ß√£o para exportar.")
            return
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("# HIST√ìRICO DE PERGUNTAS E RESPOSTAS\n\n")
            f.write(f"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n")
            f.write(f"Total de intera√ß√µes: {len(self.conversation_history)}\n\n")
            f.write("="*70 + "\n\n")
            
            for i, conv in enumerate(self.conversation_history, 1):
                f.write(f"## Intera√ß√£o {i}\n\n")
                f.write(f"**Timestamp:** {conv['timestamp']}\n\n")
                f.write(f"**Pergunta:**\n{conv['question']}\n\n")
                f.write(f"**Resposta:**\n{conv['answer']}\n\n")
                f.write("-"*70 + "\n\n")
        
        print(f"‚úÖ Log exportado para: {filepath}")


# Fun√ß√µes auxiliares para uso facilitado

def interactive_qa_session(qa_system: QASystem):
    """
    Inicia uma sess√£o interativa de perguntas e respostas.
    
    Args:
        qa_system: Inst√¢ncia configurada do QASystem
    """
    print("\n" + "="*70)
    print("üí¨ SESS√ÉO INTERATIVA DE PERGUNTAS E RESPOSTAS")
    print("="*70)
    print("\nComandos especiais:")
    print("  ‚Ä¢ 'sair' ou 'exit' - Encerra a sess√£o")
    print("  ‚Ä¢ 'melhorias' - Gera sugest√µes de melhoria")
    print("  ‚Ä¢ 'gargalos' - Identifica gargalos")
    print("  ‚Ä¢ 'historico' - Mostra hist√≥rico de perguntas")
    print("  ‚Ä¢ 'limpar' - Limpa hist√≥rico")
    print("\nFa√ßa suas perguntas sobre as rotas otimizadas:\n")
    
    while True:
        try:
            question = input("\nüí¨ Voc√™: ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['sair', 'exit', 'quit']:
                print("\nüëã Encerrando sess√£o. At√© logo!")
                break
            
            if question.lower() == 'melhorias':
                print("\nü§ñ Assistente:")
                print(qa_system.suggest_improvements())
                continue
            
            if question.lower() == 'gargalos':
                print("\nü§ñ Assistente:")
                print(qa_system.find_bottlenecks())
                continue
            
            if question.lower() == 'historico':
                history = qa_system.get_conversation_history()
                print(f"\nüìú Hist√≥rico: {len(history)} intera√ß√µes")
                for i, conv in enumerate(history, 1):
                    print(f"\n{i}. {conv['question'][:50]}...")
                continue
            
            if question.lower() == 'limpar':
                qa_system.clear_history()
                continue
            
            # Pergunta normal
            print("\nü§ñ Assistente:")
            response = qa_system.ask(question)
            print(response)
        
        except KeyboardInterrupt:
            print("\n\nüëã Sess√£o interrompida. At√© logo!")
            break
        
        except Exception as e:
            print(f"\n‚ùå Erro: {e}")
            continue

